{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the images\n",
    "imagePaths = list(paths.list_images('dataset'))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "\t# extract the class label from the filename\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\t# load the input image (224x224) and preprocess it\n",
    "\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\timage = img_to_array(image)\n",
    "\timage = preprocess_input(image)\n",
    "\t# update the data and labels lists, respectively\n",
    "\tdata.append(image)\n",
    "\tlabels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 42s 4us/step\n"
     ]
    }
   ],
   "source": [
    "# using mobinetv2 pre trained model\n",
    "\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "\tinput_shape=(224, 224, 3))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding \n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "\ttest_size=0.20, stratify=labels, random_state=42)\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=20,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "95/95 [==============================] - 99s 1s/step - loss: 0.5192 - acc: 0.7343 - val_loss: 0.3505 - val_acc: 0.8435\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 82s 867ms/step - loss: 0.2810 - acc: 0.8837 - val_loss: 0.2745 - val_acc: 0.8748\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 87s 920ms/step - loss: 0.2413 - acc: 0.9061 - val_loss: 0.2324 - val_acc: 0.8957\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 81s 856ms/step - loss: 0.1916 - acc: 0.9252 - val_loss: 0.2657 - val_acc: 0.8866\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 29s 302ms/step - loss: 0.1735 - acc: 0.9339 - val_loss: 0.2345 - val_acc: 0.8957\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 28s 293ms/step - loss: 0.1617 - acc: 0.9420 - val_loss: 0.2276 - val_acc: 0.9048\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 29s 303ms/step - loss: 0.1443 - acc: 0.9501 - val_loss: 0.2195 - val_acc: 0.9100\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.1312 - acc: 0.9510 - val_loss: 0.2208 - val_acc: 0.9100\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 28s 293ms/step - loss: 0.1303 - acc: 0.9548 - val_loss: 0.2182 - val_acc: 0.9100\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 28s 300ms/step - loss: 0.1299 - acc: 0.9525 - val_loss: 0.2658 - val_acc: 0.8983\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 30s 315ms/step - loss: 0.1254 - acc: 0.9520 - val_loss: 0.2271 - val_acc: 0.9048\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 31s 329ms/step - loss: 0.1050 - acc: 0.9597 - val_loss: 0.1593 - val_acc: 0.9361\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.1048 - acc: 0.9614 - val_loss: 0.2887 - val_acc: 0.8944\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 31s 327ms/step - loss: 0.1133 - acc: 0.9562 - val_loss: 0.2414 - val_acc: 0.9061\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 30s 314ms/step - loss: 0.0960 - acc: 0.9674 - val_loss: 0.2416 - val_acc: 0.9061\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 28s 295ms/step - loss: 0.1165 - acc: 0.9579 - val_loss: 0.3265 - val_acc: 0.8787\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 30s 313ms/step - loss: 0.0971 - acc: 0.9661 - val_loss: 0.3095 - val_acc: 0.8879\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 29s 308ms/step - loss: 0.0889 - acc: 0.9663 - val_loss: 0.2945 - val_acc: 0.8957\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 30s 320ms/step - loss: 0.0852 - acc: 0.9680 - val_loss: 0.3104 - val_acc: 0.8892\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 29s 305ms/step - loss: 0.0895 - acc: 0.9680 - val_loss: 0.3523 - val_acc: 0.8840\n"
     ]
    }
   ],
   "source": [
    "# training our model\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tvalidation_steps=len(testX) // BS,\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the trained model\n",
    "model.save('mask_recog_ver2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\JOEMATHEWS\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "model = load_model(\"mask_recog_ver2.h5\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    faces_list=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for pred in preds:\n",
    "            (mask, withoutMask) = pred\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-52oirelq\\opencv\\modules\\core\\src\\ocl.cpp:5515: error: (-220:Unknown error code -220) OpenCL error CL_MEM_OBJECT_ALLOCATION_FAILURE (-4) during call: clEnqueueWriteBuffer(q, handle=000001810A6971E0, CL_TRUE, offset=0, sz=134016, data=000001815FF24040, 0, 0, 0) in function 'cv::ocl::OpenCLAllocator::upload'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6803dfc6dce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                          \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                          \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                                          flags=cv2.CASCADE_SCALE_IMAGE)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mlocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-52oirelq\\opencv\\modules\\core\\src\\ocl.cpp:5515: error: (-220:Unknown error code -220) OpenCL error CL_MEM_OBJECT_ALLOCATION_FAILURE (-4) during call: clEnqueueWriteBuffer(q, handle=000001810A6971E0, CL_TRUE, offset=0, sz=134016, data=000001815FF24040, 0, 0, 0) in function 'cv::ocl::OpenCLAllocator::upload'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "model = load_model(\"mask_recog_ver2.h5\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    locs=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for (box, pred) in zip(locs, preds):\n",
    "                 # unpack the bounding box and predictions\n",
    "            (startX, startY, endX, endY) = box\n",
    "            (mask, withoutMask,notproper) = pred\n",
    "\n",
    "            # determine the class label and color we'll use to draw\n",
    "            # the bounding box and text\n",
    "            if (mask > withoutMask and mask>notproper):\n",
    "                label = \"Without Mask\"\n",
    "            elif ( withoutMask > notproper and withoutMask > mask):\n",
    "                label = \"Mask\"\n",
    "            else:\n",
    "                label = \"Wear Mask Properly\"\n",
    "\n",
    "            if label == \"Mask\":\n",
    "                color = (0, 255, 0)\n",
    "            elif label==\"Without Mask\":\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                color = (255, 140, 0)\n",
    "\n",
    "            # include the probability in the label\n",
    "            label = \"{}: {:.2f}%\".format(label,\n",
    "                                         max(mask, withoutMask, notproper) * 100)\n",
    "\n",
    "            # display the label and bounding box rectangle on the output\n",
    "            # frame\n",
    "            cv2.putText(frame, label, (startX, startY - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
